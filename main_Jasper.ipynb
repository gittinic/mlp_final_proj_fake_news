{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# from settings import *\n",
    "# import analyze_cascade\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from random import choices\n",
    "import math\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#metadata_file = 'metadata_anon.txt' for Nic\n",
    "metadata_file = '/Users/jaspermeijering/Google Drive/a Study/EPA Study Abroad - Carnegie Mellon University/Courses/CMU - 95845 - Applied Analytics The Machine Learning Pipeline/Machine Learning Pipeline Final Project/Data/FalseNews_Code_Data/data/metadata_anon.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read meta data \n",
    "fin = open(metadata_file,'r')\n",
    "lines = fin.readlines()\n",
    "fin.close()\n",
    "cascade_id2metadata={}\n",
    "for line in lines:\n",
    "    line = line.replace('\\n','')\n",
    "    item = eval(line)\n",
    "    cascade_id2metadata[item[0]] = item[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptives of dynamic measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breadth</th>\n",
       "      <th>category</th>\n",
       "      <th>cid</th>\n",
       "      <th>depth</th>\n",
       "      <th>engangement</th>\n",
       "      <th>nfollowees</th>\n",
       "      <th>nfollowers</th>\n",
       "      <th>size</th>\n",
       "      <th>veracity</th>\n",
       "      <th>verified</th>\n",
       "      <th>virality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10703</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>106998</td>\n",
       "      <td>11</td>\n",
       "      <td>25.799399</td>\n",
       "      <td>186.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>23228</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>4.003857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11783</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>106999</td>\n",
       "      <td>9</td>\n",
       "      <td>10.811974</td>\n",
       "      <td>313.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>14827</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>2.535338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6504</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>107000</td>\n",
       "      <td>13</td>\n",
       "      <td>15.395237</td>\n",
       "      <td>518.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>14129</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>4.019705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5772</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>107001</td>\n",
       "      <td>8</td>\n",
       "      <td>3.140842</td>\n",
       "      <td>189.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>9972</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>3.271008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6041</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>107002</td>\n",
       "      <td>8</td>\n",
       "      <td>5.160261</td>\n",
       "      <td>174.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>9526</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>3.115942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   breadth                            category     cid  depth  engangement  \\\n",
       "0    10703  Viral Photos/Stories/Urban Legends  106998     11    25.799399   \n",
       "1    11783     Science/Nature/Tech/Food/Health  106999      9    10.811974   \n",
       "2     6504  Viral Photos/Stories/Urban Legends  107000     13    15.395237   \n",
       "3     5772  Viral Photos/Stories/Urban Legends  107001      8     3.140842   \n",
       "4     6041  Viral Photos/Stories/Urban Legends  107002      8     5.160261   \n",
       "\n",
       "   nfollowees  nfollowers   size veracity verified  virality  \n",
       "0       186.0       672.0  23228    MIXED    False  4.003857  \n",
       "1       313.0       380.0  14827    MIXED    False  2.535338  \n",
       "2       518.0       504.0  14129    MIXED    False  4.019705  \n",
       "3       189.0       228.0   9972    MIXED    False  3.271008  \n",
       "4       174.0       110.0   9526    MIXED    False  3.115942  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get static measures\n",
    "veracity = []\n",
    "virality = []\n",
    "depth = []\n",
    "breadth = []\n",
    "size = []\n",
    "verified = []\n",
    "nfollowers = []\n",
    "nfollowees = []\n",
    "engagement = []\n",
    "category = []\n",
    "cascadeID= []\n",
    "for cascade,metadata in cascade_id2metadata.items():\n",
    "    if metadata['virality'] is not None: \n",
    "        veracity.append(metadata['veracity'])\n",
    "        virality.append(metadata['virality'])\n",
    "        depth.append(metadata['depth'])\n",
    "        breadth.append(metadata['max_breadth'])\n",
    "        size.append(metadata['size'])\n",
    "        verified.append(metadata['verified_list'][0])\n",
    "        nfollowers.append(metadata['num_followers_list'][0])\n",
    "        nfollowees.append(metadata['num_followees_list'][0])\n",
    "        engagement.append(metadata['engagement_list'][0])\n",
    "        category.append(metadata['rumor_category'])\n",
    "        cascadeID.append(cascade) # for merging models\n",
    "\n",
    "# Convert to data frame\n",
    "df = pd.DataFrame({'cid':cascadeID,'veracity': veracity, 'virality': virality, 'depth': depth, 'breadth': breadth, 'size': size, 'verified': verified, 'nfollowers': nfollowers, \n",
    "                   'nfollowees': nfollowees, 'engangement': engagement, 'category': category})\n",
    "\n",
    "# Inspect\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth2breadth</th>\n",
       "      <th>depth2time</th>\n",
       "      <th>depth2uu</th>\n",
       "      <th>num_followees_list</th>\n",
       "      <th>uu2time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42081.000000</td>\n",
       "      <td>42081.000000</td>\n",
       "      <td>42081.000000</td>\n",
       "      <td>42081.000000</td>\n",
       "      <td>42081.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.707707</td>\n",
       "      <td>1.707707</td>\n",
       "      <td>1.707707</td>\n",
       "      <td>93.878829</td>\n",
       "      <td>93.878829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.319555</td>\n",
       "      <td>1.319555</td>\n",
       "      <td>1.319555</td>\n",
       "      <td>950.694376</td>\n",
       "      <td>950.694376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>46895.000000</td>\n",
       "      <td>46895.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>46895.000000</td>\n",
       "      <td>46895.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       depth2breadth   depth2time       depth2uu  num_followees_list  \\\n",
       "count   42081.000000  42081.000000  42081.000000        42081.000000   \n",
       "mean        1.707707      1.707707      1.707707           93.878829   \n",
       "std         1.319555      1.319555      1.319555          950.694376   \n",
       "min         1.000000      1.000000      1.000000            2.000000   \n",
       "25%         1.000000      1.000000      1.000000            2.000000   \n",
       "50%         1.000000      1.000000      1.000000            4.000000   \n",
       "75%         2.000000      2.000000      2.000000            9.000000   \n",
       "100%       24.000000     24.000000     24.000000        46895.000000   \n",
       "max        24.000000     24.000000     24.000000        46895.000000   \n",
       "\n",
       "            uu2time  \n",
       "count  42081.000000  \n",
       "mean      93.878829  \n",
       "std      950.694376  \n",
       "min        2.000000  \n",
       "25%        2.000000  \n",
       "50%        4.000000  \n",
       "75%        9.000000  \n",
       "100%   46895.000000  \n",
       "max    46895.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_depth2time = []\n",
    "len_num_followees_list = []\n",
    "len_depth2uu = []\n",
    "len_uu2time = []\n",
    "len_depth2breadth = []\n",
    "for cascade,metadata in cascade_id2metadata.items():\n",
    "    if metadata['virality'] is not None: \n",
    "        len_depth2time.append(len(metadata['depth2time'].keys()))\n",
    "        len_num_followees_list.append(len(metadata['num_followees_list']))\n",
    "        len_depth2uu.append(len(metadata['depth2uu'].keys()))\n",
    "        len_uu2time.append(len(metadata['uu2time'].keys()))\n",
    "        len_depth2breadth.append(len(metadata['depth2breadth'].keys()))\n",
    "    \n",
    "# Convert to data frame\n",
    "df_len = pd.DataFrame({'depth2time ': len_depth2time, \n",
    "                       'num_followees_list': len_num_followees_list, \n",
    "                       'depth2uu': len_depth2uu, \n",
    "                       'uu2time': len_uu2time, \n",
    "                       'depth2breadth': len_depth2breadth})\n",
    "\n",
    "# # Get summary\n",
    "df_len.describe(percentiles = [0.25, 0.5, 0.75, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSTM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dynamic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get expression of each item in a dictionary entry\n",
    "def get_expression_list(entry):\n",
    "    expression = []\n",
    "    for i in entry.keys():\n",
    "        expression.append(float(entry[i]))\n",
    "    return expression\n",
    "\n",
    "# Convert y to classification\n",
    "def veracity_to_categorical(v):\n",
    "    if v == 'FALSE':\n",
    "        vbin = [1,0,0]\n",
    "    elif v == 'MIXED':\n",
    "        vbin = [0,1,0]\n",
    "    elif v == 'TRUE':\n",
    "        vbin = [0,0,1]\n",
    "    return vbin\n",
    "\n",
    "# Get data in list format\n",
    "data = []\n",
    "for cascade,metadata in cascade_id2metadata.items():\n",
    "    if metadata['virality'] is not None:       \n",
    "        # Get depth\n",
    "        depth2time = get_expression_list(metadata['depth2time'])\n",
    "        depth2uu = get_expression_list(metadata['depth2uu'])\n",
    "        depth2breadth = get_expression_list(metadata['depth2breadth']) \n",
    "        veracity = veracity_to_categorical(metadata['veracity'])\n",
    "        data_id = []\n",
    "        for time, uu, breadth in zip(depth2time, depth2uu, depth2breadth):\n",
    "            data_t = [cascade, \n",
    "                      veracity,\n",
    "                      time, uu, breadth]\n",
    "            data_id.append(data_t)\n",
    "        data.extend([data_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: Create training and test set\n",
    "def split_list(lst, train_size): # train_size is a proportion\n",
    "    split = len(lst) * train_size\n",
    "    if split.is_integer():\n",
    "        split = int(split)\n",
    "        return lst[:split], lst[split:]\n",
    "    else:\n",
    "        split = math.floor(split) + 1\n",
    "        return lst[:split], lst[split:]\n",
    "    \n",
    "# Function: Padding for groups of equal batches\n",
    "def padding(lst, bsize):\n",
    "    if len(lst) % bsize != 0:\n",
    "        psize = bsize - (len(lst) % 5)\n",
    "        samples = choices(lst, k=psize)\n",
    "        lst.extend(samples)\n",
    "    return lst\n",
    "\n",
    "# Get sublist\n",
    "def get_sublist(list_in_list, start, stop):\n",
    "    x = []\n",
    "    for lst in list_in_list:\n",
    "        x_id = []\n",
    "        for sublist in lst:\n",
    "            if stop is None:\n",
    "                x_id.append(sublist[start:])\n",
    "            elif start is None:\n",
    "                x_id.append(sublist[:stop])\n",
    "            else:\n",
    "                x_id.append(sublist[start:stop])\n",
    "        x.extend([x_id])\n",
    "    return x\n",
    "\n",
    "# Separate id, x and y\n",
    "def separate(list_in_list):\n",
    "    cid = []\n",
    "    y = []\n",
    "    for lst in list_in_list:\n",
    "        cid.append(lst[0][0]) # only one id is needed\n",
    "#         veracity_id = []\n",
    "#         for sublist in lst:\n",
    "#             veracity_id.extend([sublist[1]])\n",
    "#         veracity.append(veracity_id)\n",
    "        y.append(lst[0][1])\n",
    "    x = get_sublist(list_in_list,2,None)\n",
    "    return cid, y, x\n",
    "\n",
    "# # Group by sequence length and append to have batches of 5 for both training and test\n",
    "data.sort(key=len)   # Randomly reshuffle before? random.shuffle(...)\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "cid_train = []\n",
    "cid_test = []\n",
    "for k, g in groupby(data, len):\n",
    "    group = list(g)\n",
    "    if len(group) > 2: # This omits too small groups\n",
    "        shuffle(group)\n",
    "        # Create train and test bucket\n",
    "        train_group, test_group = split_list(group, 0.5)\n",
    "        # Padd for equal batch size\n",
    "        train_group_padded = padding(train_group, 5)\n",
    "        test_group_padded = padding(test_group, 5)\n",
    "        # Separate list\n",
    "        cid_train_group, y_train_group, x_train_group = separate(train_group)\n",
    "        cid_test_group, y_test_group, x_test_group = separate(test_group)\n",
    "        # Append:  convert y and x into numpy array\n",
    "        x_train.append(np.array(x_train_group))\n",
    "        x_test.append(np.array(x_test_group))\n",
    "        y_train.append(np.array(y_train_group))\n",
    "        y_test.append(np.array(y_test_group))\n",
    "        cid_train.append(cid_train_group)\n",
    "        cid_test.append(cid_test_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to standardize the list\n",
    "def standardization(lst, index, mean, std):\n",
    "    for array3d in lst:\n",
    "        for array2d in array3d:\n",
    "            for vector in array2d:\n",
    "                vector[index] = (vector[index] - mean) / std\n",
    "    return lst\n",
    "\n",
    "# Function to compute mean and std of variable and then standardizes this variable in list\n",
    "def standardize_data(a_list, b_list, index):\n",
    "    var = []\n",
    "    # Compute mean and std from train data variable\n",
    "    for array3d in a_list:\n",
    "        for array2d in array3d:\n",
    "            for vector in array2d:\n",
    "                var.append(vector[index])\n",
    "    var = np.array(var)\n",
    "    var_mean = var.mean()\n",
    "    var_std = var.std()\n",
    "    # Standardize a\n",
    "    a_list_std = standardization(a_list, index, var_mean, var_std)\n",
    "    b_list_std = standardization(b_list, index, var_mean, var_std)\n",
    "    return a_list_std, b_list_std\n",
    "\n",
    "# Standardize all variables\n",
    "def standardize_all(a_list, b_list):\n",
    "    length = len(a_list[0][0][0])\n",
    "    indices = list(range(length))\n",
    "    for i in indices:\n",
    "        std_a, std_b = standardize_data(a_list, b_list, i)\n",
    "    return std_a, std_b\n",
    "\n",
    "x_train, x_test = standardize_all(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM train data descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  1   Observations:  12960  Sequence length 1\n",
      "Group:  2   Observations:  4790  Sequence length 2\n",
      "Group:  3   Observations:  1790  Sequence length 3\n",
      "Group:  4   Observations:  760  Sequence length 4\n",
      "Group:  5   Observations:  320  Sequence length 5\n",
      "Group:  6   Observations:  170  Sequence length 6\n",
      "Group:  7   Observations:  95  Sequence length 7\n",
      "Group:  8   Observations:  65  Sequence length 8\n",
      "Group:  9   Observations:  35  Sequence length 9\n",
      "Group:  10   Observations:  25  Sequence length 10\n",
      "Group:  11   Observations:  20  Sequence length 11\n",
      "Group:  12   Observations:  15  Sequence length 12\n",
      "Group:  13   Observations:  10  Sequence length 13\n",
      "Group:  14   Observations:  10  Sequence length 14\n",
      "Group:  15   Observations:  5  Sequence length 15\n",
      "Group:  16   Observations:  5  Sequence length 16\n",
      "Group:  17   Observations:  5  Sequence length 17\n",
      "Group:  18   Observations:  5  Sequence length 19\n"
     ]
    }
   ],
   "source": [
    "# Group size and sequence length\n",
    "i = 1\n",
    "for g in x_train:\n",
    "    print('Group: ', i, ' ', 'Observations: ', len(g), ' ' 'Sequence length', len(g[0]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  1 Counter({'FALSE': 9703, 'TRUE': 2005, 'MIXED': 1252})\n",
      "Group:  2 Counter({'FALSE': 3663, 'TRUE': 679, 'MIXED': 448})\n",
      "Group:  3 Counter({'FALSE': 1383, 'TRUE': 244, 'MIXED': 163})\n",
      "Group:  4 Counter({'FALSE': 592, 'TRUE': 109, 'MIXED': 59})\n",
      "Group:  5 Counter({'FALSE': 251, 'TRUE': 41, 'MIXED': 28})\n",
      "Group:  6 Counter({'FALSE': 127, 'TRUE': 25, 'MIXED': 18})\n",
      "Group:  7 Counter({'FALSE': 87, 'TRUE': 5, 'MIXED': 3})\n",
      "Group:  8 Counter({'FALSE': 52, 'TRUE': 7, 'MIXED': 6})\n",
      "Group:  9 Counter({'FALSE': 31, 'MIXED': 2, 'TRUE': 2})\n",
      "Group:  10 Counter({'FALSE': 20, 'MIXED': 4, 'TRUE': 1})\n",
      "Group:  11 Counter({'FALSE': 20})\n",
      "Group:  12 Counter({'FALSE': 14, 'TRUE': 1})\n",
      "Group:  13 Counter({'FALSE': 8, 'MIXED': 2})\n",
      "Group:  14 Counter({'FALSE': 8, 'MIXED': 2})\n",
      "Group:  15 Counter({'FALSE': 5})\n",
      "Group:  16 Counter({'FALSE': 5})\n",
      "Group:  17 Counter({'FALSE': 5})\n",
      "Group:  18 Counter({'FALSE': 4, 'MIXED': 1})\n"
     ]
    }
   ],
   "source": [
    "# Convert y to classification\n",
    "def reverse_veracity_to_categorical(vbin):\n",
    "    if vbin[0] == 1:\n",
    "        v = 'FALSE'\n",
    "    elif vbin[1] == 1:\n",
    "        v = 'MIXED'\n",
    "    elif vbin[2] == 1:\n",
    "        v = 'TRUE'\n",
    "    return v\n",
    "\n",
    "# Outcome distribution\n",
    "i = 1\n",
    "for g in y_train:\n",
    "    ver = []\n",
    "    for y in g:\n",
    "        ver.append(reverse_veracity_to_categorical(y))\n",
    "    print('Group: ', i, Counter(ver))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM on dynamic measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, input_shape = (None, 3),  return_sequences = False))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        ..., \n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        ..., \n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        ..., \n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        ..., \n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]), array([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "12960/12960 [==============================] - 8s 584us/step - loss: 0.7579 - acc: 0.7474\n",
      "Epoch 2/2\n",
      "12960/12960 [==============================] - 8s 615us/step - loss: 0.7320 - acc: 0.7487\n",
      "Epoch 1/2\n",
      "4790/4790 [==============================] - 4s 760us/step - loss: 0.7081 - acc: 0.7647\n",
      "Epoch 2/2\n",
      "4790/4790 [==============================] - 4s 780us/step - loss: 0.7063 - acc: 0.7647\n",
      "Epoch 1/2\n",
      "1790/1790 [==============================] - 2s 942us/step - loss: 0.6904 - acc: 0.7726\n",
      "Epoch 2/2\n",
      "1790/1790 [==============================] - 2s 965us/step - loss: 0.6897 - acc: 0.7726\n",
      "Epoch 1/2\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.6772 - acc: 0.7789\n",
      "Epoch 2/2\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.6748 - acc: 0.7789\n",
      "Epoch 1/2\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.6819 - acc: 0.7844\n",
      "Epoch 2/2\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.6741 - acc: 0.7844\n",
      "Epoch 1/2\n",
      "170/170 [==============================] - 0s 1ms/step - loss: 0.7505 - acc: 0.7471\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 0s 1ms/step - loss: 0.7402 - acc: 0.7471\n",
      "Epoch 1/2\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.4055 - acc: 0.9158\n",
      "Epoch 2/2\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.3737 - acc: 0.9158\n",
      "Epoch 1/2\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.6367 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.6344 - acc: 0.8000\n",
      "Epoch 1/2\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.4215 - acc: 0.8857\n",
      "Epoch 2/2\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.4208 - acc: 0.8857\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6893 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6831 - acc: 0.8000\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1452 - acc: 1.0000\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1438 - acc: 1.0000\n",
      "Epoch 1/2\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2891 - acc: 0.9333\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2898 - acc: 0.9333\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6070 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6030 - acc: 0.8000\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7609 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7578 - acc: 0.8000\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1115 - acc: 1.0000\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1117 - acc: 1.0000\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1568 - acc: 1.0000\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1570 - acc: 1.0000\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1212 - acc: 1.0000\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1208 - acc: 1.0000\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6986 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6981 - acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Fit model and get train predictions\n",
    "train_pred = []\n",
    "for X,Y in zip(x_train, y_train):\n",
    "    hist = model.fit(X, Y, epochs=2, batch_size=5)\n",
    "    pred = model.predict(X, batch_size=5)\n",
    "    train_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert predictions to data frame with ID\n",
    "cid = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "y3 = []\n",
    "for pred_group in train_pred:\n",
    "    for pred in pred_group:\n",
    "        y1.append(pred[0])\n",
    "        y2.append(pred[1])\n",
    "        y3.append(pred[2])\n",
    "        \n",
    "for cid_group in cid_train:\n",
    "    for i in cid_group:\n",
    "        cid.append(i)\n",
    "\n",
    "ytrain_pred = pd.DataFrame({'cid': cid, 'y1': y1, 'y2': y2, 'y3': y3})\n",
    "ytrain_pred = ytrain_pred.drop_duplicates('cid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21042"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ytrain_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_old = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(ytrain_pred, df_old, how = 'left',on = 'cid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>breadth</th>\n",
       "      <th>category</th>\n",
       "      <th>depth</th>\n",
       "      <th>engangement</th>\n",
       "      <th>nfollowees</th>\n",
       "      <th>nfollowers</th>\n",
       "      <th>size</th>\n",
       "      <th>veracity</th>\n",
       "      <th>verified</th>\n",
       "      <th>virality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67369</td>\n",
       "      <td>0.737633</td>\n",
       "      <td>0.102950</td>\n",
       "      <td>0.159417</td>\n",
       "      <td>5</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>5</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51793</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>29.768926</td>\n",
       "      <td>3836.0</td>\n",
       "      <td>3928.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109235</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>27.736943</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>3</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55869</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>16.749603</td>\n",
       "      <td>307.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86566</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>1</td>\n",
       "      <td>4.046090</td>\n",
       "      <td>128.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cid        y1        y2        y3  breadth  \\\n",
       "0   67369  0.737633  0.102950  0.159417        5   \n",
       "1   51793  0.739528  0.102038  0.158435        2   \n",
       "2  109235  0.738897  0.102341  0.158762        3   \n",
       "3   55869  0.739528  0.102038  0.158435        2   \n",
       "4   86566  0.739528  0.102038  0.158435        2   \n",
       "\n",
       "                             category  depth  engangement  nfollowees  \\\n",
       "0             War/Terrorism/Shootings      1   325.577912     20670.0   \n",
       "1                            Politics      1    29.768926      3836.0   \n",
       "2                            Politics      1    27.736943      1013.0   \n",
       "3                            Business      1    16.749603       307.0   \n",
       "4  Viral Photos/Stories/Urban Legends      1     4.046090       128.0   \n",
       "\n",
       "   nfollowers  size veracity verified  virality  \n",
       "0     41239.0     5    FALSE    False  1.600000  \n",
       "1      3928.0     2    FALSE    False  1.000000  \n",
       "2       960.0     3    MIXED    False  1.333333  \n",
       "3       331.0     2    FALSE    False  1.000000  \n",
       "4       137.0     2     TRUE    False  1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = merged_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical and Continious model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>breadth</th>\n",
       "      <th>category</th>\n",
       "      <th>depth</th>\n",
       "      <th>engangement</th>\n",
       "      <th>nfollowees</th>\n",
       "      <th>nfollowers</th>\n",
       "      <th>size</th>\n",
       "      <th>veracity</th>\n",
       "      <th>verified</th>\n",
       "      <th>virality</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67369</td>\n",
       "      <td>0.737633</td>\n",
       "      <td>0.102950</td>\n",
       "      <td>0.159417</td>\n",
       "      <td>5</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>5</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51793</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>29.768926</td>\n",
       "      <td>3836.0</td>\n",
       "      <td>3928.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109235</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>27.736943</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>3</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55869</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>16.749603</td>\n",
       "      <td>307.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86566</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>1</td>\n",
       "      <td>4.046090</td>\n",
       "      <td>128.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>66590</td>\n",
       "      <td>0.737633</td>\n",
       "      <td>0.102950</td>\n",
       "      <td>0.159417</td>\n",
       "      <td>5</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>5</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61789</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>8.711250</td>\n",
       "      <td>371.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>3</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63289</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>1</td>\n",
       "      <td>137.401393</td>\n",
       "      <td>1105.0</td>\n",
       "      <td>3831.0</td>\n",
       "      <td>3</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85517</td>\n",
       "      <td>0.738265</td>\n",
       "      <td>0.102645</td>\n",
       "      <td>0.159089</td>\n",
       "      <td>4</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>12.300890</td>\n",
       "      <td>1539.0</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>4</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85644</td>\n",
       "      <td>0.738265</td>\n",
       "      <td>0.102645</td>\n",
       "      <td>0.159089</td>\n",
       "      <td>4</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>14.074003</td>\n",
       "      <td>2603.0</td>\n",
       "      <td>2361.0</td>\n",
       "      <td>4</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>53512</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>12.182356</td>\n",
       "      <td>7429.0</td>\n",
       "      <td>6864.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>62449</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>32.400091</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1566.0</td>\n",
       "      <td>3</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>69034</td>\n",
       "      <td>0.735730</td>\n",
       "      <td>0.103868</td>\n",
       "      <td>0.160402</td>\n",
       "      <td>8</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>8</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>111071</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>1</td>\n",
       "      <td>29.888121</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>1256.0</td>\n",
       "      <td>2</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>54346</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>1</td>\n",
       "      <td>3.687151</td>\n",
       "      <td>496.0</td>\n",
       "      <td>14521.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>86447</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>9.281045</td>\n",
       "      <td>1828.0</td>\n",
       "      <td>14076.0</td>\n",
       "      <td>2</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>109863</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>1</td>\n",
       "      <td>96.983035</td>\n",
       "      <td>7120.0</td>\n",
       "      <td>11149.0</td>\n",
       "      <td>2</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>64660</td>\n",
       "      <td>0.738265</td>\n",
       "      <td>0.102645</td>\n",
       "      <td>0.159089</td>\n",
       "      <td>4</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>4</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>111015</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>1</td>\n",
       "      <td>3.148929</td>\n",
       "      <td>4776.0</td>\n",
       "      <td>20999.0</td>\n",
       "      <td>2</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>86005</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>46.645076</td>\n",
       "      <td>7143.0</td>\n",
       "      <td>7279.0</td>\n",
       "      <td>3</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>63725</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>3</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>52007</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>4.574825</td>\n",
       "      <td>838.0</td>\n",
       "      <td>9863.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>108216</td>\n",
       "      <td>0.732542</td>\n",
       "      <td>0.105410</td>\n",
       "      <td>0.162048</td>\n",
       "      <td>13</td>\n",
       "      <td>Natural Disasters</td>\n",
       "      <td>1</td>\n",
       "      <td>1.441039</td>\n",
       "      <td>136.0</td>\n",
       "      <td>230638.0</td>\n",
       "      <td>13</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>True</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>55983</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Natural Disasters</td>\n",
       "      <td>1</td>\n",
       "      <td>3.828164</td>\n",
       "      <td>988.0</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>59566</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>57058</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>1</td>\n",
       "      <td>3.819978</td>\n",
       "      <td>202.0</td>\n",
       "      <td>3653.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>84567</td>\n",
       "      <td>0.733181</td>\n",
       "      <td>0.105101</td>\n",
       "      <td>0.161718</td>\n",
       "      <td>12</td>\n",
       "      <td>Politics</td>\n",
       "      <td>1</td>\n",
       "      <td>83.414304</td>\n",
       "      <td>35388.0</td>\n",
       "      <td>66915.0</td>\n",
       "      <td>12</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>62286</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>2.056990</td>\n",
       "      <td>793.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>3</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>62340</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>3</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>1</td>\n",
       "      <td>12.865503</td>\n",
       "      <td>5906.0</td>\n",
       "      <td>5570.0</td>\n",
       "      <td>3</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>58889</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>2</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>1</td>\n",
       "      <td>30.371797</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>2833.0</td>\n",
       "      <td>2</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21012</th>\n",
       "      <td>82601</td>\n",
       "      <td>0.871480</td>\n",
       "      <td>0.053482</td>\n",
       "      <td>0.075038</td>\n",
       "      <td>317</td>\n",
       "      <td>Politics</td>\n",
       "      <td>12</td>\n",
       "      <td>85.605152</td>\n",
       "      <td>6096.0</td>\n",
       "      <td>13417.0</td>\n",
       "      <td>1032</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>False</td>\n",
       "      <td>5.723650</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21013</th>\n",
       "      <td>82405</td>\n",
       "      <td>0.916305</td>\n",
       "      <td>0.037387</td>\n",
       "      <td>0.046308</td>\n",
       "      <td>962</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>12</td>\n",
       "      <td>11.580062</td>\n",
       "      <td>265.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>3359</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>5.859105</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21014</th>\n",
       "      <td>82085</td>\n",
       "      <td>0.860614</td>\n",
       "      <td>0.056905</td>\n",
       "      <td>0.082481</td>\n",
       "      <td>230</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>12</td>\n",
       "      <td>0.443353</td>\n",
       "      <td>71.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>889</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>5.725007</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21015</th>\n",
       "      <td>107009</td>\n",
       "      <td>0.805567</td>\n",
       "      <td>0.109047</td>\n",
       "      <td>0.085387</td>\n",
       "      <td>1484</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>13</td>\n",
       "      <td>3.146312</td>\n",
       "      <td>239.0</td>\n",
       "      <td>91836.0</td>\n",
       "      <td>3464</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>4.428582</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21016</th>\n",
       "      <td>82400</td>\n",
       "      <td>0.919796</td>\n",
       "      <td>0.036122</td>\n",
       "      <td>0.044082</td>\n",
       "      <td>868</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>13</td>\n",
       "      <td>25.094254</td>\n",
       "      <td>752.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>3271</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>5.938564</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21017</th>\n",
       "      <td>82055</td>\n",
       "      <td>0.851696</td>\n",
       "      <td>0.059586</td>\n",
       "      <td>0.088718</td>\n",
       "      <td>157</td>\n",
       "      <td>Politics</td>\n",
       "      <td>13</td>\n",
       "      <td>11.893461</td>\n",
       "      <td>7335.0</td>\n",
       "      <td>40164.0</td>\n",
       "      <td>721</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>7.514817</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21018</th>\n",
       "      <td>107000</td>\n",
       "      <td>0.842991</td>\n",
       "      <td>0.063102</td>\n",
       "      <td>0.093907</td>\n",
       "      <td>6504</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>13</td>\n",
       "      <td>15.395237</td>\n",
       "      <td>518.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>14129</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>4.019705</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21019</th>\n",
       "      <td>82307</td>\n",
       "      <td>0.912421</td>\n",
       "      <td>0.039571</td>\n",
       "      <td>0.048009</td>\n",
       "      <td>451</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>13</td>\n",
       "      <td>91.480315</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>14245.0</td>\n",
       "      <td>2062</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>7.282017</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21020</th>\n",
       "      <td>82521</td>\n",
       "      <td>0.902248</td>\n",
       "      <td>0.042109</td>\n",
       "      <td>0.055643</td>\n",
       "      <td>2783</td>\n",
       "      <td>Politics</td>\n",
       "      <td>13</td>\n",
       "      <td>20.989461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7620</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>None</td>\n",
       "      <td>5.078605</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21021</th>\n",
       "      <td>82445</td>\n",
       "      <td>0.917976</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.044445</td>\n",
       "      <td>812</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>14</td>\n",
       "      <td>63.516232</td>\n",
       "      <td>4101.0</td>\n",
       "      <td>28562.0</td>\n",
       "      <td>4148</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>7.862370</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21022</th>\n",
       "      <td>82570</td>\n",
       "      <td>0.788558</td>\n",
       "      <td>0.096443</td>\n",
       "      <td>0.114999</td>\n",
       "      <td>9640</td>\n",
       "      <td>Politics</td>\n",
       "      <td>14</td>\n",
       "      <td>6.199575</td>\n",
       "      <td>25.0</td>\n",
       "      <td>301575.0</td>\n",
       "      <td>19011</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>3.923420</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21023</th>\n",
       "      <td>82558</td>\n",
       "      <td>0.842588</td>\n",
       "      <td>0.064840</td>\n",
       "      <td>0.092572</td>\n",
       "      <td>6829</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>14</td>\n",
       "      <td>14.121360</td>\n",
       "      <td>296.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>13574</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>3.874699</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21024</th>\n",
       "      <td>82420</td>\n",
       "      <td>0.920136</td>\n",
       "      <td>0.036518</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>923</td>\n",
       "      <td>Politics</td>\n",
       "      <td>14</td>\n",
       "      <td>4.950415</td>\n",
       "      <td>470.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>3484</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.356373</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21025</th>\n",
       "      <td>82467</td>\n",
       "      <td>0.920070</td>\n",
       "      <td>0.036875</td>\n",
       "      <td>0.043055</td>\n",
       "      <td>1371</td>\n",
       "      <td>Politics</td>\n",
       "      <td>14</td>\n",
       "      <td>8.585074</td>\n",
       "      <td>59.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4826</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>5.503420</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21026</th>\n",
       "      <td>107004</td>\n",
       "      <td>0.913208</td>\n",
       "      <td>0.039136</td>\n",
       "      <td>0.047657</td>\n",
       "      <td>2110</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>14</td>\n",
       "      <td>20.554899</td>\n",
       "      <td>542.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>6203</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>5.110521</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21027</th>\n",
       "      <td>82550</td>\n",
       "      <td>0.855175</td>\n",
       "      <td>0.061363</td>\n",
       "      <td>0.083462</td>\n",
       "      <td>4105</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>14</td>\n",
       "      <td>57.789697</td>\n",
       "      <td>723.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>11240</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>4.958810</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21028</th>\n",
       "      <td>82514</td>\n",
       "      <td>0.851528</td>\n",
       "      <td>0.078526</td>\n",
       "      <td>0.069947</td>\n",
       "      <td>4075</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>14</td>\n",
       "      <td>2.760212</td>\n",
       "      <td>219.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>6914</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>4.074938</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21029</th>\n",
       "      <td>82472</td>\n",
       "      <td>0.922384</td>\n",
       "      <td>0.035713</td>\n",
       "      <td>0.041903</td>\n",
       "      <td>1887</td>\n",
       "      <td>War/Terrorism/Shootings</td>\n",
       "      <td>15</td>\n",
       "      <td>70.303469</td>\n",
       "      <td>264.0</td>\n",
       "      <td>1801.0</td>\n",
       "      <td>4883</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>4.890189</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21030</th>\n",
       "      <td>82119</td>\n",
       "      <td>0.871268</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.074787</td>\n",
       "      <td>225</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>15</td>\n",
       "      <td>5.521592</td>\n",
       "      <td>865.0</td>\n",
       "      <td>31766.0</td>\n",
       "      <td>791</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.466821</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21031</th>\n",
       "      <td>82531</td>\n",
       "      <td>0.887553</td>\n",
       "      <td>0.049310</td>\n",
       "      <td>0.063137</td>\n",
       "      <td>1707</td>\n",
       "      <td>Politics</td>\n",
       "      <td>15</td>\n",
       "      <td>12.575665</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3661.0</td>\n",
       "      <td>8386</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.964480</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21032</th>\n",
       "      <td>82482</td>\n",
       "      <td>0.920755</td>\n",
       "      <td>0.036266</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>1576</td>\n",
       "      <td>Politics</td>\n",
       "      <td>15</td>\n",
       "      <td>4.284413</td>\n",
       "      <td>812.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>5206</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.103439</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21033</th>\n",
       "      <td>82562</td>\n",
       "      <td>0.834099</td>\n",
       "      <td>0.068952</td>\n",
       "      <td>0.096948</td>\n",
       "      <td>2924</td>\n",
       "      <td>Politics</td>\n",
       "      <td>16</td>\n",
       "      <td>23.662750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>14733</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>7.403186</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21034</th>\n",
       "      <td>82532</td>\n",
       "      <td>0.884826</td>\n",
       "      <td>0.051669</td>\n",
       "      <td>0.063505</td>\n",
       "      <td>2315</td>\n",
       "      <td>Politics</td>\n",
       "      <td>16</td>\n",
       "      <td>1.745002</td>\n",
       "      <td>474.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>8394</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.373106</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21035</th>\n",
       "      <td>82552</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.060916</td>\n",
       "      <td>0.081429</td>\n",
       "      <td>4747</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>16</td>\n",
       "      <td>1.948094</td>\n",
       "      <td>161.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>11182</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>4.785286</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21036</th>\n",
       "      <td>82567</td>\n",
       "      <td>0.841256</td>\n",
       "      <td>0.064678</td>\n",
       "      <td>0.094066</td>\n",
       "      <td>5164</td>\n",
       "      <td>Viral Photos/Stories/Urban Legends</td>\n",
       "      <td>16</td>\n",
       "      <td>21.274198</td>\n",
       "      <td>393.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>16700</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.062984</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21037</th>\n",
       "      <td>82469</td>\n",
       "      <td>0.923173</td>\n",
       "      <td>0.036106</td>\n",
       "      <td>0.040721</td>\n",
       "      <td>1724</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>17</td>\n",
       "      <td>25.817683</td>\n",
       "      <td>361.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>4659</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>5.428979</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21038</th>\n",
       "      <td>82580</td>\n",
       "      <td>0.862993</td>\n",
       "      <td>0.052637</td>\n",
       "      <td>0.084370</td>\n",
       "      <td>6777</td>\n",
       "      <td>Politics</td>\n",
       "      <td>17</td>\n",
       "      <td>22.680557</td>\n",
       "      <td>155.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>26525</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.536872</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21039</th>\n",
       "      <td>82435</td>\n",
       "      <td>0.918579</td>\n",
       "      <td>0.039649</td>\n",
       "      <td>0.041772</td>\n",
       "      <td>751</td>\n",
       "      <td>Politics</td>\n",
       "      <td>19</td>\n",
       "      <td>1.228952</td>\n",
       "      <td>94.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3339</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>7.821709</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21040</th>\n",
       "      <td>107026</td>\n",
       "      <td>0.881864</td>\n",
       "      <td>0.051861</td>\n",
       "      <td>0.066275</td>\n",
       "      <td>238</td>\n",
       "      <td>Science/Nature/Tech/Food/Health</td>\n",
       "      <td>19</td>\n",
       "      <td>21.730338</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>20870.0</td>\n",
       "      <td>1049</td>\n",
       "      <td>MIXED</td>\n",
       "      <td>False</td>\n",
       "      <td>10.068822</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21041</th>\n",
       "      <td>82586</td>\n",
       "      <td>0.863817</td>\n",
       "      <td>0.052684</td>\n",
       "      <td>0.083498</td>\n",
       "      <td>10566</td>\n",
       "      <td>Politics</td>\n",
       "      <td>19</td>\n",
       "      <td>66.207298</td>\n",
       "      <td>153.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>42149</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>False</td>\n",
       "      <td>6.716822</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21042 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cid        y1        y2        y3  breadth  \\\n",
       "0       67369  0.737633  0.102950  0.159417        5   \n",
       "1       51793  0.739528  0.102038  0.158435        2   \n",
       "2      109235  0.738897  0.102341  0.158762        3   \n",
       "3       55869  0.739528  0.102038  0.158435        2   \n",
       "4       86566  0.739528  0.102038  0.158435        2   \n",
       "5       66590  0.737633  0.102950  0.159417        5   \n",
       "6       61789  0.738897  0.102341  0.158762        3   \n",
       "7       63289  0.738897  0.102341  0.158762        3   \n",
       "8       85517  0.738265  0.102645  0.159089        4   \n",
       "9       85644  0.738265  0.102645  0.159089        4   \n",
       "10      53512  0.739528  0.102038  0.158435        2   \n",
       "11      62449  0.738897  0.102341  0.158762        3   \n",
       "12      69034  0.735730  0.103868  0.160402        8   \n",
       "13     111071  0.739528  0.102038  0.158435        2   \n",
       "14      54346  0.739528  0.102038  0.158435        2   \n",
       "15      86447  0.739528  0.102038  0.158435        2   \n",
       "16     109863  0.739528  0.102038  0.158435        2   \n",
       "17      64660  0.738265  0.102645  0.159089        4   \n",
       "18     111015  0.739528  0.102038  0.158435        2   \n",
       "19      86005  0.738897  0.102341  0.158762        3   \n",
       "20      63725  0.738897  0.102341  0.158762        3   \n",
       "21      52007  0.739528  0.102038  0.158435        2   \n",
       "22     108216  0.732542  0.105410  0.162048       13   \n",
       "23      55983  0.739528  0.102038  0.158435        2   \n",
       "24      59566  0.739528  0.102038  0.158435        2   \n",
       "25      57058  0.739528  0.102038  0.158435        2   \n",
       "26      84567  0.733181  0.105101  0.161718       12   \n",
       "27      62286  0.738897  0.102341  0.158762        3   \n",
       "28      62340  0.738897  0.102341  0.158762        3   \n",
       "29      58889  0.739528  0.102038  0.158435        2   \n",
       "...       ...       ...       ...       ...      ...   \n",
       "21012   82601  0.871480  0.053482  0.075038      317   \n",
       "21013   82405  0.916305  0.037387  0.046308      962   \n",
       "21014   82085  0.860614  0.056905  0.082481      230   \n",
       "21015  107009  0.805567  0.109047  0.085387     1484   \n",
       "21016   82400  0.919796  0.036122  0.044082      868   \n",
       "21017   82055  0.851696  0.059586  0.088718      157   \n",
       "21018  107000  0.842991  0.063102  0.093907     6504   \n",
       "21019   82307  0.912421  0.039571  0.048009      451   \n",
       "21020   82521  0.902248  0.042109  0.055643     2783   \n",
       "21021   82445  0.917976  0.037579  0.044445      812   \n",
       "21022   82570  0.788558  0.096443  0.114999     9640   \n",
       "21023   82558  0.842588  0.064840  0.092572     6829   \n",
       "21024   82420  0.920136  0.036518  0.043347      923   \n",
       "21025   82467  0.920070  0.036875  0.043055     1371   \n",
       "21026  107004  0.913208  0.039136  0.047657     2110   \n",
       "21027   82550  0.855175  0.061363  0.083462     4105   \n",
       "21028   82514  0.851528  0.078526  0.069947     4075   \n",
       "21029   82472  0.922384  0.035713  0.041903     1887   \n",
       "21030   82119  0.871268  0.053945  0.074787      225   \n",
       "21031   82531  0.887553  0.049310  0.063137     1707   \n",
       "21032   82482  0.920755  0.036266  0.042979     1576   \n",
       "21033   82562  0.834099  0.068952  0.096948     2924   \n",
       "21034   82532  0.884826  0.051669  0.063505     2315   \n",
       "21035   82552  0.857656  0.060916  0.081429     4747   \n",
       "21036   82567  0.841256  0.064678  0.094066     5164   \n",
       "21037   82469  0.923173  0.036106  0.040721     1724   \n",
       "21038   82580  0.862993  0.052637  0.084370     6777   \n",
       "21039   82435  0.918579  0.039649  0.041772      751   \n",
       "21040  107026  0.881864  0.051861  0.066275      238   \n",
       "21041   82586  0.863817  0.052684  0.083498    10566   \n",
       "\n",
       "                                 category  depth  engangement  nfollowees  \\\n",
       "0                 War/Terrorism/Shootings      1   325.577912     20670.0   \n",
       "1                                Politics      1    29.768926      3836.0   \n",
       "2                                Politics      1    27.736943      1013.0   \n",
       "3                                Business      1    16.749603       307.0   \n",
       "4      Viral Photos/Stories/Urban Legends      1     4.046090       128.0   \n",
       "5                                Politics      1   325.577912     20670.0   \n",
       "6                                Politics      1     8.711250       371.0   \n",
       "7         Science/Nature/Tech/Food/Health      1   137.401393      1105.0   \n",
       "8                                Politics      1    12.300890      1539.0   \n",
       "9                                Politics      1    14.074003      2603.0   \n",
       "10                               Politics      1    12.182356      7429.0   \n",
       "11                               Business      1    32.400091        74.0   \n",
       "12                               Politics      1   325.577912     20670.0   \n",
       "13        Science/Nature/Tech/Food/Health      1    29.888121      1805.0   \n",
       "14                          Entertainment      1     3.687151       496.0   \n",
       "15                               Politics      1     9.281045      1828.0   \n",
       "16                War/Terrorism/Shootings      1    96.983035      7120.0   \n",
       "17                               Politics      1   325.577912     20670.0   \n",
       "18     Viral Photos/Stories/Urban Legends      1     3.148929      4776.0   \n",
       "19                               Politics      1    46.645076      7143.0   \n",
       "20                War/Terrorism/Shootings      1   325.577912     20670.0   \n",
       "21                               Politics      1     4.574825       838.0   \n",
       "22                      Natural Disasters      1     1.441039       136.0   \n",
       "23                      Natural Disasters      1     3.828164       988.0   \n",
       "24                War/Terrorism/Shootings      1   325.577912     20670.0   \n",
       "25        Science/Nature/Tech/Food/Health      1     3.819978       202.0   \n",
       "26                               Politics      1    83.414304     35388.0   \n",
       "27                               Business      1     2.056990       793.0   \n",
       "28                          Entertainment      1    12.865503      5906.0   \n",
       "29                War/Terrorism/Shootings      1    30.371797      3193.0   \n",
       "...                                   ...    ...          ...         ...   \n",
       "21012                            Politics     12    85.605152      6096.0   \n",
       "21013             War/Terrorism/Shootings     12    11.580062       265.0   \n",
       "21014     Science/Nature/Tech/Food/Health     12     0.443353        71.0   \n",
       "21015  Viral Photos/Stories/Urban Legends     13     3.146312       239.0   \n",
       "21016  Viral Photos/Stories/Urban Legends     13    25.094254       752.0   \n",
       "21017                            Politics     13    11.893461      7335.0   \n",
       "21018  Viral Photos/Stories/Urban Legends     13    15.395237       518.0   \n",
       "21019  Viral Photos/Stories/Urban Legends     13    91.480315      1614.0   \n",
       "21020                            Politics     13    20.989461         NaN   \n",
       "21021  Viral Photos/Stories/Urban Legends     14    63.516232      4101.0   \n",
       "21022                            Politics     14     6.199575        25.0   \n",
       "21023                       Entertainment     14    14.121360       296.0   \n",
       "21024                            Politics     14     4.950415       470.0   \n",
       "21025                            Politics     14     8.585074        59.0   \n",
       "21026  Viral Photos/Stories/Urban Legends     14    20.554899       542.0   \n",
       "21027  Viral Photos/Stories/Urban Legends     14    57.789697       723.0   \n",
       "21028     Science/Nature/Tech/Food/Health     14     2.760212       219.0   \n",
       "21029             War/Terrorism/Shootings     15    70.303469       264.0   \n",
       "21030                       Entertainment     15     5.521592       865.0   \n",
       "21031                            Politics     15    12.575665       302.0   \n",
       "21032                            Politics     15     4.284413       812.0   \n",
       "21033                            Politics     16    23.662750         2.0   \n",
       "21034                            Politics     16     1.745002       474.0   \n",
       "21035     Science/Nature/Tech/Food/Health     16     1.948094       161.0   \n",
       "21036  Viral Photos/Stories/Urban Legends     16    21.274198       393.0   \n",
       "21037     Science/Nature/Tech/Food/Health     17    25.817683       361.0   \n",
       "21038                            Politics     17    22.680557       155.0   \n",
       "21039                            Politics     19     1.228952        94.0   \n",
       "21040     Science/Nature/Tech/Food/Health     19    21.730338      1556.0   \n",
       "21041                            Politics     19    66.207298       153.0   \n",
       "\n",
       "       nfollowers   size veracity verified   virality train_test  \n",
       "0         41239.0      5    FALSE    False   1.600000      train  \n",
       "1          3928.0      2    FALSE    False   1.000000      train  \n",
       "2           960.0      3    MIXED    False   1.333333      train  \n",
       "3           331.0      2    FALSE    False   1.000000      train  \n",
       "4           137.0      2     TRUE    False   1.000000      train  \n",
       "5         41239.0      5    FALSE    False   1.600000      train  \n",
       "6           965.0      3    FALSE    False   1.333333      train  \n",
       "7          3831.0      3    FALSE    False   1.333333      train  \n",
       "8          1460.0      4     TRUE    False   1.500000      train  \n",
       "9          2361.0      4     TRUE    False   1.500000      train  \n",
       "10         6864.0      2    FALSE    False   1.000000      train  \n",
       "11         1566.0      3    FALSE    False   1.333333      train  \n",
       "12        41239.0      8    FALSE    False   1.750000      train  \n",
       "13         1256.0      2    MIXED    False   1.000000      train  \n",
       "14        14521.0      2    FALSE    False   1.000000      train  \n",
       "15        14076.0      2     TRUE     True   1.000000      train  \n",
       "16        11149.0      2    MIXED    False   1.000000      train  \n",
       "17        41239.0      4    FALSE    False   1.500000      train  \n",
       "18        20999.0      2    MIXED     True   1.000000      train  \n",
       "19         7279.0      3     TRUE    False   1.333333      train  \n",
       "20        41239.0      3    FALSE    False   1.333333      train  \n",
       "21         9863.0      2    FALSE    False   1.000000      train  \n",
       "22       230638.0     13    MIXED     True   1.846154      train  \n",
       "23         1549.0      2    FALSE    False   1.000000      train  \n",
       "24        41239.0      2    FALSE    False   1.000000      train  \n",
       "25         3653.0      2    FALSE    False   1.000000      train  \n",
       "26        66915.0     12     TRUE    False   1.833333      train  \n",
       "27         2191.0      3    FALSE    False   1.333333      train  \n",
       "28         5570.0      3    FALSE    False   1.333333      train  \n",
       "29         2833.0      2    FALSE    False   1.000000      train  \n",
       "...           ...    ...      ...      ...        ...        ...  \n",
       "21012     13417.0   1032     TRUE    False   5.723650      train  \n",
       "21013       433.0   3359    FALSE    False   5.859105      train  \n",
       "21014       976.0    889    FALSE    False   5.725007      train  \n",
       "21015     91836.0   3464    MIXED    False   4.428582      train  \n",
       "21016       439.0   3271    FALSE    False   5.938564      train  \n",
       "21017     40164.0    721    FALSE    False   7.514817      train  \n",
       "21018       504.0  14129    MIXED    False   4.019705      train  \n",
       "21019     14245.0   2062    FALSE    False   7.282017      train  \n",
       "21020         NaN   7620    FALSE     None   5.078605      train  \n",
       "21021     28562.0   4148    FALSE    False   7.862370      train  \n",
       "21022    301575.0  19011    FALSE    False   3.923420      train  \n",
       "21023       457.0  13574    FALSE    False   3.874699      train  \n",
       "21024       385.0   3484    FALSE    False   6.356373      train  \n",
       "21025       100.0   4826    FALSE    False   5.503420      train  \n",
       "21026       716.0   6203    MIXED    False   5.110521      train  \n",
       "21027       879.0  11240    FALSE    False   4.958810      train  \n",
       "21028       207.0   6914    FALSE    False   4.074938      train  \n",
       "21029      1801.0   4883    FALSE    False   4.890189      train  \n",
       "21030     31766.0    791    FALSE    False   6.466821      train  \n",
       "21031      3661.0   8386    FALSE    False   6.964480      train  \n",
       "21032       767.0   5206    FALSE    False   6.103439      train  \n",
       "21033       688.0  14733    FALSE    False   7.403186      train  \n",
       "21034       452.0   8394    FALSE    False   6.373106      train  \n",
       "21035       109.0  11182    FALSE    False   4.785286      train  \n",
       "21036       633.0  16700    FALSE    False   6.062984      train  \n",
       "21037       523.0   4659    FALSE    False   5.428979      train  \n",
       "21038       230.0  26525    FALSE    False   6.536872      train  \n",
       "21039        63.0   3339    FALSE    False   7.821709      train  \n",
       "21040     20870.0   1049    MIXED    False  10.068822      train  \n",
       "21041       269.0  42149    FALSE    False   6.716822      train  \n",
       "\n",
       "[21042 rows x 15 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"train_test\"] = \"train\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_saved = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing: Make data numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed categories to numeric values, this is how they match [War/Terrorism/Shootings, Politics, Business, Viral Photos/Stories/Urban Legends, Science/Nature/Tech/Food/Health, Entertainment, Natural Disasters]\n",
      "Categories (7, object): [War/Terrorism/Shootings, Politics, Business, Viral Photos/Stories/Urban Legends, Science/Nature/Tech/Food/Health, Entertainment, Natural Disasters] [7 4 1 6 5 2 3]\n",
      "Transformed veracity to numeric values, this is how they match [FALSE, MIXED, TRUE]\n",
      "Categories (3, object): [FALSE, MIXED, TRUE] [1 2 3]\n",
      "Transformed verified to numeric values, this is how they match [False True None] [-9223372036854775808]\n"
     ]
    }
   ],
   "source": [
    "# Make category numeric\n",
    "df[\"category\"] = df[\"category\"].astype('category')\n",
    "df[\"category_num\"] = df.category.cat.rename_categories([1,2,3,4,5,6,7])\n",
    "df[\"category_num\"] = df[\"category_num\"].astype('int')\n",
    "print(\"Transformed categories to numeric values, this is how they match\",df.category.unique(),df.category_num.unique())\n",
    "\n",
    "# Make veracity numeric\n",
    "df[\"veracity\"] = df[\"veracity\"].astype('category')\n",
    "df[\"veracity_num\"] = df.veracity.cat.rename_categories([1,2,3])\n",
    "df[\"veracity_num\"] = df[\"veracity_num\"].astype('int')\n",
    "print(\"Transformed veracity to numeric values, this is how they match\",df.veracity.unique(),df.veracity_num.unique())\n",
    "\n",
    "# Make verified numeric\n",
    "df[\"verified_cat\"] = df[\"verified\"].astype('category')\n",
    "df[\"verified_cat\"] = df[\"verified_cat\"].cat.set_categories([\"True\",\"False\",\"None\"])\n",
    "#df[\"verified_cat\"].isnull().sum() # 136 nan / none values for verified\n",
    "#df.loc[df[\"verified_cat\"].isnull(),'verified'] = \"None\" # -> leave these cascades in. # this is wrong\n",
    "df[\"verified_num_pre\"] = df.verified_cat.cat.rename_categories([3,2,1])\n",
    "df[\"verified_num\"] = df[\"verified_num_pre\"].astype('int')\n",
    "print(\"Transformed verified to numeric values, this is how they match\",df.verified.unique(),df.verified_num.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>veracity_num</th>\n",
       "      <th>breadth</th>\n",
       "      <th>category_num</th>\n",
       "      <th>depth</th>\n",
       "      <th>engangement</th>\n",
       "      <th>nfollowees</th>\n",
       "      <th>nfollowers</th>\n",
       "      <th>size</th>\n",
       "      <th>verified_num</th>\n",
       "      <th>virality</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67369</td>\n",
       "      <td>0.737633</td>\n",
       "      <td>0.102950</td>\n",
       "      <td>0.159417</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>325.577912</td>\n",
       "      <td>20670.0</td>\n",
       "      <td>41239.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51793</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29.768926</td>\n",
       "      <td>3836.0</td>\n",
       "      <td>3928.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109235</td>\n",
       "      <td>0.738897</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>27.736943</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55869</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.749603</td>\n",
       "      <td>307.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86566</td>\n",
       "      <td>0.739528</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>0.158435</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4.046090</td>\n",
       "      <td>128.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-9223372036854775808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cid        y1        y2        y3  veracity_num  breadth  category_num  \\\n",
       "0   67369  0.737633  0.102950  0.159417             1        5             7   \n",
       "1   51793  0.739528  0.102038  0.158435             1        2             4   \n",
       "2  109235  0.738897  0.102341  0.158762             2        3             4   \n",
       "3   55869  0.739528  0.102038  0.158435             1        2             1   \n",
       "4   86566  0.739528  0.102038  0.158435             3        2             6   \n",
       "\n",
       "   depth  engangement  nfollowees  nfollowers  size         verified_num  \\\n",
       "0      1   325.577912     20670.0     41239.0     5 -9223372036854775808   \n",
       "1      1    29.768926      3836.0      3928.0     2 -9223372036854775808   \n",
       "2      1    27.736943      1013.0       960.0     3 -9223372036854775808   \n",
       "3      1    16.749603       307.0       331.0     2 -9223372036854775808   \n",
       "4      1     4.046090       128.0       137.0     2 -9223372036854775808   \n",
       "\n",
       "   virality train_test  \n",
       "0  1.600000      train  \n",
       "1  1.000000      train  \n",
       "2  1.333333      train  \n",
       "3  1.000000      train  \n",
       "4  1.000000      train  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep relevant columns\n",
    "column_names_df = ['cid','y1','y2','y3','veracity_num','breadth', 'category_num', 'depth', 'engangement','nfollowees', 'nfollowers', 'size', 'verified_num', 'virality','train_test']\n",
    "column_names_x =['cid','y1','y2','y3','breadth', 'category_num', 'depth', 'engangement','nfollowees', 'nfollowers', 'size', 'verified_num', 'virality']\n",
    "column_names_x_cat = ['category_num','verified_num']  # add cid?\n",
    "column_names_x_con = ['y1','y2','y3','breadth', 'depth', 'engangement', 'nfollowees', 'nfollowers', 'size', 'virality'] #add cid\n",
    "column_names_y =['veracity_num','cid'] # ['cid','veracity_num']\n",
    "\n",
    "df = df.loc[:,column_names_df]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cat  = df.loc[df[\"train_test\"] == \"train\",column_names_x_cat ]\n",
    "X_test_cat  = df.loc[df[\"train_test\"] == \"test\",column_names_x_cat ]\n",
    "X_train_con  = df.loc[df[\"train_test\"] == \"train\",column_names_x_con ]\n",
    "X_test_con  = df.loc[df[\"train_test\"] == \"test\",column_names_x_con ]\n",
    "Y_train = df.loc[df[\"train_test\"] == \"train\",column_names_y ]\n",
    "Y_test = df.loc[df[\"train_test\"] == \"test\",column_names_y ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity_num</th>\n",
       "      <th>cid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>67369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veracity_num     cid\n",
       "0             1   67369\n",
       "1             1   51793\n",
       "2             2  109235"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_Y_train = df.loc[df[\"train_test\"] == \"train\",column_names_y ]\n",
    "save_Y_test = df.loc[df[\"train_test\"] == \"train\",column_names_y ]\n",
    "save_Y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_Y_train.head(10)\n",
    "def change_formatY(save_Y_train):\n",
    "    save_Y_train.index = np.arange(1, len(save_Y_train) + 1)\n",
    "    for a in range(3):\n",
    "        save_Y_train[\"Y_input_%d\"% (a+1)] = 0\n",
    "    for i in range(len(save_Y_train)):\n",
    "        for j in range(4):\n",
    "            if save_Y_train.iloc[i,0] == j:\n",
    "                save_Y_train[\"Y_input_%d\"% (j)][i+1] = 1\n",
    "                break\n",
    "    return(save_Y_train.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_input_1</th>\n",
       "      <th>Y_input_2</th>\n",
       "      <th>Y_input_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y_input_1  Y_input_2  Y_input_3\n",
       "1          1          0          0\n",
       "2          1          0          0\n",
       "3          0          1          0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = change_formatY(save_Y_train)\n",
    "Y_test = change_formatY(save_Y_test)\n",
    "Y_train = Y_train.loc[:,['Y_input_1','Y_input_2','Y_input_3']] # without CID\n",
    "Y_test = Y_test.loc[:,['Y_input_1','Y_input_2','Y_input_3']] # without CID\n",
    "Y_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_formatX(X_train_cat):\n",
    "    X_train_cat.index = np.arange(1, len(X_train_cat) + 1)\n",
    "    for a in range(7):\n",
    "        X_train_cat[\"category_num_%d\"% (a+1)] = 0\n",
    "    #for b in range(31):\n",
    "     #   X_train_cat[\"day_%d\"% (b+1)] = 0\n",
    "   # for c in range(3):\n",
    "   #     X_train_cat[\"verified_%d\"% (c+1)] = 0\n",
    "    \n",
    "    for i in range(len(X_train_cat)):\n",
    "        for j in range(8):\n",
    "            if X_train_cat.iloc[i,0] == j:\n",
    "                X_train_cat[\"category_num_%d\"% (j)][i+1] = 1\n",
    "                break\n",
    "    #    for k in range(32):\n",
    "     #       if X_train_cat.iloc[i,1] == k:\n",
    "      #          X_train_cat[\"day_%d\"% (k)][i+1] = 1\n",
    "       #         break\n",
    "      #  for l in range(4):\n",
    "     #       if X_train_cat.iloc[i,2] == l:\n",
    "     #           X_train_cat[\"verified_%d\"% (l)][i+1] = 1\n",
    "     #           break     \n",
    "    return X_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat2 = change_formatX(X_train_cat)\n",
    "X_test_cat2 = change_formatX(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_num_2</th>\n",
       "      <th>category_num_3</th>\n",
       "      <th>category_num_4</th>\n",
       "      <th>category_num_5</th>\n",
       "      <th>category_num_6</th>\n",
       "      <th>category_num_7</th>\n",
       "      <th>verified_1</th>\n",
       "      <th>verified_2</th>\n",
       "      <th>verified_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_num_2  category_num_3  category_num_4  category_num_5  \\\n",
       "1               0               0               0               0   \n",
       "2               0               0               1               0   \n",
       "3               0               0               1               0   \n",
       "4               0               0               0               0   \n",
       "5               0               0               0               0   \n",
       "\n",
       "   category_num_6  category_num_7  verified_1  verified_2  verified_3  \n",
       "1               0               1           0           0           0  \n",
       "2               0               0           0           0           0  \n",
       "3               0               0           0           0           0  \n",
       "4               0               0           0           0           0  \n",
       "5               1               0           0           0           0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this block only once (or in combination with the block above)\n",
    "X_train_cat2 = X_train_cat2.iloc[:,3:] # [:,4:] for dataframe without cid [:,3:] for dataframe with cid \n",
    "X_test_cat2 = X_test_cat2.iloc[:,3:] # [:,4:] for dataframe without cid\n",
    "X_train_cat2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize continious data ONLY RUN ONCE\n",
    "X_train_con = np.log(X_train_con.iloc[:,:]+1)\n",
    "X_test_con = np.log(X_test_con.iloc[:,:]+1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change datastructue df -> matrix\n",
    "X_train_cat  = X_train_cat2.as_matrix()\n",
    "X_test_cat = X_test_cat2.as_matrix() \n",
    "X_train_con  = X_train_con.as_matrix()\n",
    "X_test_con = X_test_con.as_matrix()\n",
    "Y_train = Y_train.as_matrix()\n",
    "Y_test = Y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21042/21042 [==============================] - 1s 69us/step - loss: 0.7146 - mean_squared_error: 0.1327 - acc: 0.7566\n",
      "Epoch 2/10\n",
      "21042/21042 [==============================] - 1s 54us/step - loss: 0.6865 - mean_squared_error: 0.1270 - acc: 0.7576\n",
      "Epoch 3/10\n",
      "21042/21042 [==============================] - 1s 56us/step - loss: 0.6806 - mean_squared_error: 0.1260 - acc: 0.7576\n",
      "Epoch 4/10\n",
      "21042/21042 [==============================] - 1s 51us/step - loss: 0.6781 - mean_squared_error: 0.1257 - acc: 0.7576\n",
      "Epoch 5/10\n",
      "21042/21042 [==============================] - 1s 53us/step - loss: 0.6769 - mean_squared_error: 0.1255 - acc: 0.7576\n",
      "Epoch 6/10\n",
      "21042/21042 [==============================] - 1s 53us/step - loss: 0.6766 - mean_squared_error: 0.1255 - acc: 0.7576\n",
      "Epoch 7/10\n",
      "21042/21042 [==============================] - 1s 60us/step - loss: 0.6762 - mean_squared_error: 0.1255 - acc: 0.7576\n",
      "Epoch 8/10\n",
      "21042/21042 [==============================] - 1s 55us/step - loss: 0.6760 - mean_squared_error: 0.1255 - acc: 0.7576\n",
      "Epoch 9/10\n",
      "21042/21042 [==============================] - 1s 52us/step - loss: 0.6761 - mean_squared_error: 0.1255 - acc: 0.7576\n",
      "Epoch 10/10\n",
      "21042/21042 [==============================] - 1s 50us/step - loss: 0.6758 - mean_squared_error: 0.1254 - acc: 0.7576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1857f29710>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(9,)) # 10 with verified\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(4, activation='relu')(inputs)\n",
    "x = Dense(4, activation='relu')(x)\n",
    "x = Dense(4, activation='tanh')(x)\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model_cat_new = Model(inputs=inputs, outputs=predictions)\n",
    "model_cat_new.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['mse','accuracy']) #accuracy\n",
    "model_cat_new.fit(X_train_cat, Y_train,epochs=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 40        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 95\n",
      "Trainable params: 95\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cat_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJEAAAHBCAIAAADmQ843AAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nCGR/hh/EGX4QZ/hBnOHH/wChAEq7c7pA8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model as image\n",
    "plot_model(model_cat_new, to_file='Images/model_cat_new.png')\n",
    "Image(\"Images/model_cat_new.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_con = Sequential()\n",
    "\n",
    "model_con.add(Dense(4,input_dim = 10,activation = 'relu')) \n",
    "\n",
    "model_con.add(Dense(40, activation = 'tanh' ))\n",
    "model_con.add(Dense(40, activation = 'relu' ))\n",
    "model_con.add(Dense(3, activation = 'softmax' ))\n",
    "\n",
    "model_con.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_con, to_file='Images/model_con.png')\n",
    "Image(\"Images/model_con.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 4)                 44        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                200       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 123       \n",
      "=================================================================\n",
      "Total params: 2,007\n",
      "Trainable params: 2,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_con.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21042/21042 [==============================] - 1s 33us/step - loss: nan - acc: 0.7534\n",
      "Epoch 2/20\n",
      "21042/21042 [==============================] - 0s 13us/step - loss: nan - acc: 0.7576\n",
      "Epoch 3/20\n",
      "21042/21042 [==============================] - 0s 14us/step - loss: nan - acc: 0.7576\n",
      "Epoch 4/20\n",
      "21042/21042 [==============================] - 0s 15us/step - loss: nan - acc: 0.7576\n",
      "Epoch 5/20\n",
      "21042/21042 [==============================] - 0s 13us/step - loss: nan - acc: 0.7576\n",
      "Epoch 6/20\n",
      "21042/21042 [==============================] - 0s 14us/step - loss: nan - acc: 0.7576\n",
      "Epoch 7/20\n",
      "21042/21042 [==============================] - 0s 16us/step - loss: nan - acc: 0.7576\n",
      "Epoch 8/20\n",
      "21042/21042 [==============================] - 0s 14us/step - loss: nan - acc: 0.7576\n",
      "Epoch 9/20\n",
      "21042/21042 [==============================] - 0s 13us/step - loss: nan - acc: 0.7576\n",
      "Epoch 10/20\n",
      "21042/21042 [==============================] - 0s 14us/step - loss: nan - acc: 0.7576\n",
      "Epoch 11/20\n",
      "21042/21042 [==============================] - 0s 15us/step - loss: nan - acc: 0.7576\n",
      "Epoch 12/20\n",
      "21042/21042 [==============================] - 0s 15us/step - loss: nan - acc: 0.7576\n",
      "Epoch 13/20\n",
      "21042/21042 [==============================] - 0s 15us/step - loss: nan - acc: 0.7576\n",
      "Epoch 14/20\n",
      "21042/21042 [==============================] - 0s 15us/step - loss: nan - acc: 0.7576\n",
      "Epoch 15/20\n",
      "21042/21042 [==============================] - 0s 16us/step - loss: nan - acc: 0.7576\n",
      "Epoch 16/20\n",
      "21042/21042 [==============================] - 0s 14us/step - loss: nan - acc: 0.7576\n",
      "Epoch 17/20\n",
      "21042/21042 [==============================] - 0s 17us/step - loss: nan - acc: 0.7576\n",
      "Epoch 18/20\n",
      "21042/21042 [==============================] - 0s 15us/step - loss: nan - acc: 0.7576\n",
      "Epoch 19/20\n",
      "21042/21042 [==============================] - 0s 16us/step - loss: nan - acc: 0.7576\n",
      "Epoch 20/20\n",
      "21042/21042 [==============================] - 0s 16us/step - loss: nan - acc: 0.7576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18581346d8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_con.fit(X_train_con, Y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21042/21042 [==============================] - 0s 8us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75762760188232614"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get test_set from Nic\n",
    "#score_con = model_con.evaluate(X_test_con, Y_test, batch_size=128)\n",
    "score_con = model_con.evaluate(X_train_con, Y_train, batch_size=128)\n",
    "score_con[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_input = Input(shape=(9,), name='Categorial_input')  #10 with verified\n",
    "con_input = Input(shape=(10,), name='Continous_input')\n",
    "\n",
    "#lstm_input = ??\n",
    "\n",
    "x = keras.layers.concatenate([cat_input, con_input])\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "main_output = Dense(3, activation='softmax', name='main_output')(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_total = Model(inputs=[cat_input, con_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Categorial_input (InputLayer)   (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Continous_input (InputLayer)    (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 19)           0           Categorial_input[0][0]           \n",
      "                                                                 Continous_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 64)           1280        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 64)           4160        dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 64)           4160        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 3)            195         dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,795\n",
      "Trainable params: 9,795\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_total.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAIjCAYAAADLIIbOAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model_total, to_file='Images/model_combined.png')\n",
    "from IPython.display import Image\n",
    "Image(\"Images/model_combined.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_total.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['mse','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21042/21042 [==============================] - 2s 78us/step - loss: nan - mean_squared_error: nan - acc: 0.7575\n",
      "Epoch 2/20\n",
      "21042/21042 [==============================] - 1s 59us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 3/20\n",
      "21042/21042 [==============================] - 1s 59us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 4/20\n",
      "21042/21042 [==============================] - 1s 55us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 5/20\n",
      "21042/21042 [==============================] - 1s 57us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 6/20\n",
      "21042/21042 [==============================] - 1s 58us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 7/20\n",
      "21042/21042 [==============================] - 1s 58us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 8/20\n",
      "21042/21042 [==============================] - 1s 57us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 9/20\n",
      "21042/21042 [==============================] - 1s 57us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 10/20\n",
      "21042/21042 [==============================] - 1s 60us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 11/20\n",
      "21042/21042 [==============================] - 1s 65us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 12/20\n",
      "21042/21042 [==============================] - 1s 60us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 13/20\n",
      "21042/21042 [==============================] - 1s 60us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 14/20\n",
      "21042/21042 [==============================] - 1s 57us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 15/20\n",
      "21042/21042 [==============================] - 1s 55us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 16/20\n",
      "21042/21042 [==============================] - 1s 53us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 17/20\n",
      "21042/21042 [==============================] - 1s 46us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 18/20\n",
      "21042/21042 [==============================] - 1s 45us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 19/20\n",
      "21042/21042 [==============================] - 1s 45us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n",
      "Epoch 20/20\n",
      "21042/21042 [==============================] - 1s 45us/step - loss: nan - mean_squared_error: nan - acc: 0.7576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x185b752978>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_total.fit([X_train_cat, X_train_con], Y_train,\n",
    "          epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get test_set from Nic\n",
    "#score_total = model_total.evaluate(  )\n",
    "#score_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cat[1].shape\n",
    "#X_train_con[1].shape\n",
    "#X_train_con[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected Categorial_input to have shape (9,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-ac33eb1e6aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model_total.predict([X_train_cat,X_train_con])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_con\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#model_total.predict([[X_train_cat[2]],X_train_con[2]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected Categorial_input to have shape (9,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#model_total.predict([X_train_cat,X_train_con])\n",
    "\n",
    "model_total.predict([X_train_cat[2],X_train_con[2]])\n",
    "#model_total.predict([[X_train_cat[2]],X_train_con[2]])\n",
    "\n",
    "#model_total.predict(0, 0, 0, 1, 0, 0, 0, 0, 0, 0,[ 0.56294174,  0.08946883,  0.14026323,  1.38629436,  0.69314718,\n",
    "    #    2.80746823,  7.02108396,  7.28687641,  1.38629436,  0.84729786])\n",
    "\n",
    "#model_total.predict([X_train_cat[10:20],X_train_con[10:20]]) # does work, why does is needs to be 10 by 10?\n",
    "# it should out put a array of 3 not of 3 by 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to calculate the roc curve you need: roc_curve(Y_train[:, i], score_con[1]).\n",
    "# the score per row, we don't have that. But can calculate it by doing a prediction. \n",
    "#The predicion though doens't work.\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# it needs an input of 10 by 10 instead of 9 and 10\n",
    "#when you feed it 10 by 10 you only get nan (so maybe you don't neeed 10 by 10?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_con[1], Y_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(3):\n",
    "    #fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_train[:, i], score_con[1]))\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
